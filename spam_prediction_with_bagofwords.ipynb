{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW-1 Part 1. Spam Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p7Z8eeN5IW9q"
      },
      "source": [
        "# Part 1.\n",
        "\n",
        "The deadline for Part 1 is **1:30 pm Feb 6th, 2020**.   \n",
        "You should submit a `.ipynb` file with your solutions to NYU Classes.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this part we will preprocess SMS Spam Collection Dataset and train a bag-of-words classifier (logistic regression) for spam detection. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dZd0LJzbISPd"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "First, we download the SMS Spam Collection Dataset. The dataset is taken from [Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset/data#) and loaded to [Google Drive](https://drive.google.com/open?id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR) so that everyone can access it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PvGErs2oHkWU",
        "outputId": "e8285b60-ae2a-422a-97f2-a21d0786b6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!wget 'https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR' -O spam.csv "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-12 16:08:48--  https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.20.138, 74.125.20.102, 74.125.20.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.20.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p075sendvfo9q0te2bo28bvh5ad7oe4i/1581523200000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-02-12 16:08:48--  https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p075sendvfo9q0te2bo28bvh5ad7oe4i/1581523200000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download\n",
            "Resolving doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503663 (492K) [text/csv]\n",
            "Saving to: ‘spam.csv’\n",
            "\n",
            "spam.csv            100%[===================>] 491.86K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2020-02-12 16:08:48 (160 MB/s) - ‘spam.csv’ saved [503663/503663]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RcHV1lUwtH-n",
        "outputId": "8e67c31c-2c4a-4112-e90a-d933bfa42a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spam.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eXVQCF-ovo4G"
      },
      "source": [
        "There are two columns: `v1` -- spam or ham indicator, `v2` -- text of the message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BiKE89v0zMiY",
        "outputId": "9478a795-1433-4a37-9ad0-9b5883b0b325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"spam.csv\", usecols=[\"v1\", \"v2\"], encoding='latin-1')\n",
        "# 1 - spam, 0 - ham\n",
        "df.v1 = (df.v1 == \"spam\").astype(\"int\")\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   v1                                                 v2\n",
              "0   0  Go until jurong point, crazy.. Available only ...\n",
              "1   0                      Ok lar... Joking wif u oni...\n",
              "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   0  U dun say so early hor... U c already then say...\n",
              "4   0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nXQhTzrCv-Nk"
      },
      "source": [
        "Your task is to split the data to train/dev/test. Make sure that each row \n",
        "\n",
        "---\n",
        "\n",
        "appears only in one of the splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIIGB_82ZB1j",
        "colab_type": "code",
        "outputId": "0e337df5-439f-4303-d784-323f523ffe16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df['v2']\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Go until jurong point, crazy.. Available only ...\n",
              "1                           Ok lar... Joking wif u oni...\n",
              "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3       U dun say so early hor... U c already then say...\n",
              "4       Nah I don't think he goes to usf, he lives aro...\n",
              "                              ...                        \n",
              "5567    This is the 2nd time we have tried 2 contact u...\n",
              "5568                Will Ì_ b going to esplanade fr home?\n",
              "5569    Pity, * was in mood for that. So...any other s...\n",
              "5570    The guy did some bitching but I acted like i'd...\n",
              "5571                           Rofl. Its true to its name\n",
              "Name: v2, Length: 5572, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ga5Qydpw-gdQ",
        "colab": {}
      },
      "source": [
        "# 0.15 for val, 0.15 for test, 0.7 for train\n",
        "val_size = int(df.shape[0] * 0.15)\n",
        "test_size = int(df.shape[0] * 0.15)\n",
        "\n",
        "\"\"\"\n",
        "split the dataset into train/val/test\n",
        "\"\"\"\n",
        "\n",
        "df_copy = df.copy()\n",
        "train_set = df_copy.sample(frac=0.7, random_state=10)\n",
        "rest = df_copy.drop(train_set.index)\n",
        "val_set = rest.sample(frac=0.5, random_state=10)\n",
        "test_set = rest.drop(val_set.index)\n",
        "\n",
        "\n",
        "train_texts, train_labels = train_set['v2'], train_set['v1']\n",
        "val_texts, val_labels     = val_set['v2'], val_set['v1']\n",
        "test_texts, test_labels   = test_set['v2'], test_set['v1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByrKDBfPY_Zo",
        "colab_type": "code",
        "outputId": "a3bf395f-d6be-4499-f3b8-952ba9fb7346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_texts.iloc[3899]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi I'm sue. I am 20 years old and work as a lapdancer. I love sex. Text me live - I'm i my bedroom now. text SUE to 89555. By TextOperator G2 1DA 150ppmsg 18+\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k19yYB_xDEj",
        "colab_type": "code",
        "outputId": "010067b7-0275-4b92-934f-bb7abaa5c1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_set)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3900"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M72rWZ4xDPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QGyHG4lBISP2"
      },
      "source": [
        "## Data Processing\n",
        "\n",
        "The task is to create bag-of-words features: tokenize the text, index each token, represent the sentence as a dictionary of tokens and their counts, limit the vocabulary to $n$ most frequent tokens. In the lab we use built-in `sklearn` function, `sklearn.feature_extraction.text.CountVectorizer`. \n",
        "**In this HW, you are required to implement the `Vectorizer` on your own without using `sklearn` built-in functions.**\n",
        "\n",
        "Function `preprocess_data` takes the list of texts and returns list of (lists of tokens). \n",
        "You may use [spacy](https://spacy.io/) or [nltk](https://www.nltk.org/) text processing libraries in `preprocess_data` function. \n",
        "\n",
        "Class `Vectorizer` is used to vectorize the text and to create a matrix of features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "793EFaQYhHeR",
        "outputId": "b399d959-b064-4cba-bf2c-29d7ea8ea597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # This function should return a list of lists of preprocessed tokens for each message\n",
        "    # remove the punctuation\n",
        "    # remove the stop words\n",
        "    preprocessed_data = []\n",
        "    for _ in range(len(data)):\n",
        "      nonpunct = [char for char in data.iloc[_] if char not in string.punctuation]\n",
        "      nonpunct = ''.join(nonpunct)\n",
        "      sub_preprocessed_data = [word for word in nonpunct.split() if word.lower() not in stopwords.words('english')]\n",
        "      preprocessed_data.append(sub_preprocessed_data)\n",
        "    return preprocessed_data\n",
        "\n",
        "train_data = preprocess_data(train_texts)\n",
        "val_data = preprocess_data(val_texts)\n",
        "test_data = preprocess_data(test_texts)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-KervoRxAXP",
        "colab_type": "code",
        "outputId": "d5e9dfbd-1a36-49e8-eb2e-1a5b60d2c456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "train_data[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['K', 'k', 'pa', 'lunch', 'aha'],\n",
              " ['Sorry', 'Ill', 'call', 'later', 'meeting'],\n",
              " ['Never',\n",
              "  'try',\n",
              "  'alone',\n",
              "  'take',\n",
              "  'weight',\n",
              "  'tear',\n",
              "  'comes',\n",
              "  'ur',\n",
              "  'heart',\n",
              "  'falls',\n",
              "  'ur',\n",
              "  'eyes',\n",
              "  'Always',\n",
              "  'remember',\n",
              "  'STUPID',\n",
              "  'FRIEND',\n",
              "  'share',\n",
              "  'BSLVYL'],\n",
              " ['Hey', 'happy', 'birthday'],\n",
              " ['Ill', 'hand', 'phone', 'chat', 'wit', 'u']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfUwQJWSQols",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data = train_data[:5]\n",
        "# vocab = []\n",
        "# for line in data:\n",
        "#   for word in line:\n",
        "#     if word not in vocab:\n",
        "#       vocab.append(word)\n",
        "# # print(data)\n",
        "# print(len(vocab))\n",
        "\n",
        "# flat_list = [item for sublist in data for item in sublist]\n",
        "\n",
        "# num = [[x,flat_list.count(x)] for x in set(vocab)]\n",
        "# max_fea = 10\n",
        "# print(num)\n",
        "# vo = []\n",
        "# for i in range(max_fea):\n",
        "#   vo.append(num[i][0])\n",
        "  \n",
        "# print(vo)\n",
        "# vo.index('ur')\n",
        "# only keeps the most \n",
        "\n",
        "# import numpy as np\n",
        "# matrix = np.zeros((len(data), len(vocab)))\n",
        "\n",
        "# print(data)\n",
        "# print(vocab)\n",
        "\n",
        "# row = 0\n",
        "# for sent in data:\n",
        "#   for sw in sent:\n",
        "#     for j, word in enumerate(vocab):\n",
        "#       if word == sw:\n",
        "#         matrix[row,j] += 1\n",
        "#   row += 1\n",
        "# matrix "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TM2qpOKpjVbD",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Vectorizer():\n",
        "    def __init__(self, max_features):\n",
        "        self.max_features = max_features\n",
        "        self.vocab_list = []\n",
        "        self.token_to_index = None\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        # Create a vocab list, self.vocab_list, using the most frequent \"max_features\" tokens\n",
        "      \n",
        "        # Naive vocab list\n",
        "        naive_vocab = []\n",
        "        for sentance in dataset:\n",
        "          for word in sentance:\n",
        "            if word not in naive_vocab:\n",
        "              naive_vocab.append(word)\n",
        "        \n",
        "        flat_list = [item for sublist in dataset for item in sublist]\n",
        "        num = [[x,flat_list.count(x)] for x in set(naive_vocab)]\n",
        "        for i in range(self.max_features):\n",
        "          self.vocab_list.append(num[i][0])\n",
        "    \n",
        "    def token_2_index(self, token):\n",
        "        # Create a token indexer, self.token_to_index, that will return index of the token in self.vocab\n",
        "        return self.vocab_list.index(token)\n",
        "\n",
        "\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        # This function transforms text dataset into a matrix, data_matrix\n",
        "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
        "        row = 0\n",
        "\n",
        "        for sentance in dataset:\n",
        "          for sw in sentance:\n",
        "            for col, word in enumerate(self.vocab_list):\n",
        "              if sw == word:\n",
        "                data_matrix[row, col] += 1\n",
        "          row += 1\n",
        "\n",
        "        return data_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wXMrZXlZjcH7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0e967c8-ef23-481f-b744-55b93cd9413f"
      },
      "source": [
        "def count_word(dataset):\n",
        "  vocab_list = []\n",
        "  for sentance in dataset:\n",
        "          for word in sentance:\n",
        "            if word not in vocab_list:\n",
        "              vocab_list.append(word)\n",
        "  return len(vocab_list)\n",
        "\n",
        "\n",
        "# take 90% of most frequently used word into account\n",
        "frequent_rate = 1.\n",
        "all_word = count_word(train_data)\n",
        "max_features = int(frequent_rate * all_word)\n",
        "print('selected max feature num:{}, {} of the {} dataword'.format(max_features, frequent_rate, all_word))\n",
        "\n",
        "\n",
        "vectorizer = Vectorizer(max_features=max_features)\n",
        "vectorizer.fit(train_data)\n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_val = vectorizer.transform(val_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "vocab = vectorizer.vocab_list\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "selected max feature num:9215, 1.0 of the 9215 dataword\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cGLg6udky1zo"
      },
      "source": [
        "You can add more features to the feature matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s80GgEm6F5DG",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wtm7a6JWu9-3"
      },
      "source": [
        "## Model\n",
        "\n",
        "We train logistic regression model and save prediction for train, val and test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wq9stSAbAIZe",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(random_state=0, solver='liblinear')\n",
        "\n",
        "# Fit the model to training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction using the trained model\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_val_pred = model.predict(X_val)\n",
        "y_test_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3j-Abw7JOqD_"
      },
      "source": [
        "## Performance of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Akg9LvP5DGE8"
      },
      "source": [
        "Your task is to report train, val, test accuracies and F1 scores.\n",
        "**You are required to implement `accuracy_score` and `f1_score` methods without using built-in python functions.**\n",
        "\n",
        "Your model should achieve at least **0.95** test accuracy and **0.90** test F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i52ofes8GFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "chqVbKH6kZyY",
        "colab": {}
      },
      "source": [
        "def accuracy_score(y_true, y_pred): \n",
        "    # Calculate accuracy of the model's prediction\n",
        "    err = np.mean(np.abs(y_true - y_pred))\n",
        "    accuracy = 1. - err\n",
        "    return accuracy\n",
        "\n",
        "def f1_score(y_true, y_pred): \n",
        "    # Calculate F1 score of the model's prediction\n",
        "    # precision: out of all the positive prediction, what fractions are right\n",
        "    # recall : out of all the positive samples, what fractions are correct \n",
        "    precision = np.sum(y_true * y_pred) / np.sum(y_pred)\n",
        "    recall = np.sum(y_true * y_pred) / np.sum(y_true)\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MqrMw0udDD04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "74f4f2d3-d049-4fa5-a7ef-2fdb1dd5b5f5"
      },
      "source": [
        "print(f\"Training accuracy: {accuracy_score(y_train, y_train_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_train, y_train_pred):.3f}\")\n",
        "print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_val, y_val_pred):.3f}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_test, y_test_pred):.3f}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.995, F1 score: 0.983\n",
            "Validation accuracy: 0.981, F1 score: 0.927\n",
            "Test accuracy: 0.980, F1 score: 0.909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FW7P84giGgP4"
      },
      "source": [
        "**Question.**\n",
        "Is accuracy the metric that logistic regression optimizes while training? If no, which metric is optimized in logistic regression?\n",
        "\n",
        "**Your answer:** No, in logistic regression, it is the logistic loss been optimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ak0h71krLPqX"
      },
      "source": [
        "**Question.**\n",
        "In general, does having 0.99 accuracy on test means that the model is great? If no, can you give an example of a case when the accuracy is high but the model is not good? (Hint: why do we use F1 score?)\n",
        "\n",
        "**Your answer:** In genearal, a model with 0.99 accuracy is a very well model. However, if the test set have 99% positive samples and 1% negetive samples, and and model simpley just classify all te samples as positive, which indicates the model has no classification ability, yet the model can still achieve 99% accuacy. It is caused by the imbalanced of the dataset, and the F1-score can help with the class imbalance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L_RDI0qdOxwM"
      },
      "source": [
        "### Exploration of predicitons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DHR2OqYCDOxs"
      },
      "source": [
        "Show a few examples with true+predicted labels on the train and val sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5yv8GD-UGXvR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f80eb828-148f-488f-b38d-d3ce90be571d"
      },
      "source": [
        "# 1 - spam, 0 - ham\n",
        "print('------------- train data set ------------')\n",
        "print('')\n",
        "for i in range(3):\n",
        "  print('text:', train_set.iloc[i]['v2'])\n",
        "  print('true:{}, predict:{}'.format('ham' if train_set.iloc[i]['v1'] < 0.5 else 'spam',\n",
        "                                     'ham' if y_train_pred[i] < 0.5 else 'spam'))\n",
        "  print('')\n",
        "\n",
        "print('')\n",
        "\n",
        "print('------------- validation data set ------------')\n",
        "print('')\n",
        "for i in range(3):\n",
        "  print('text:', val_set.iloc[i]['v2'])\n",
        "  print('true:{}, predict:{}'.format('ham' if val_set.iloc[i]['v1'] < 0.5 else 'spam',\n",
        "                                     'ham' if y_val_pred[i] < 0.5 else 'spam'))\n",
        "  print('')\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------- train data set ------------\n",
            "\n",
            "text: K k pa Had your lunch aha.\n",
            "true:ham, predict:ham\n",
            "\n",
            "text: Sorry, I'll call later in meeting\n",
            "true:ham, predict:ham\n",
            "\n",
            "text: Never try alone to take the weight of a tear that comes out of ur heart and falls through ur eyes... Always remember a STUPID FRIEND is here to share... BSLVYL\n",
            "true:ham, predict:ham\n",
            "\n",
            "\n",
            "------------- validation data set ------------\n",
            "\n",
            "text: complimentary 4 STAR Ibiza Holiday or å£10,000 cash needs your URGENT collection. 09066364349 NOW from Landline not to lose out! Box434SK38WP150PPM18+\n",
            "true:spam, predict:ham\n",
            "\n",
            "text: Sorry completely forgot * will pop em round this week if your still here?\n",
            "true:ham, predict:ham\n",
            "\n",
            "text: When did dad get back.\n",
            "true:ham, predict:ham\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "neMQ4VR9GVL3"
      },
      "source": [
        "**Question** Print 10 examples from val set which were labeled incorrectly by the model. Why do you think the model got them wrong?\n",
        "\n",
        "> 缩进块\n",
        "\n",
        "\n",
        "\n",
        "**Your answer:** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ssK0jRxGY3u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "5bbde607-94d2-41bf-9d8e-4269b3d1f080"
      },
      "source": [
        "wrong_index = np.where(np.abs(y_val_pred - y_val)>0.5)\n",
        "\n",
        "count = 1\n",
        "for i in wrong_index[0][:10]:\n",
        "  print('example ', count)\n",
        "  count += 1\n",
        "  print(val_set.iloc[i]['v2'])\n",
        "  print('class:', val_set.iloc[i]['v1'])\n",
        "  print(' ')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example  1\n",
            "complimentary 4 STAR Ibiza Holiday or å£10,000 cash needs your URGENT collection. 09066364349 NOW from Landline not to lose out! Box434SK38WP150PPM18+\n",
            "class: 1\n",
            " \n",
            "example  2\n",
            "You have an important customer service announcement from PREMIER.\n",
            "class: 1\n",
            " \n",
            "example  3\n",
            "Save money on wedding lingerie at www.bridal.petticoatdreams.co.uk Choose from a superb selection with national delivery. Brought to you by WeddingFriend\n",
            "class: 1\n",
            " \n",
            "example  4\n",
            "Win the newest ÛÏHarry Potter and the Order of the Phoenix (Book 5) reply HARRY, answer 5 questions - chance to be the first among readers!\n",
            "class: 1\n",
            " \n",
            "example  5\n",
            "Babe: U want me dont u baby! Im nasty and have a thing 4 filthyguys. Fancy a rude time with a sexy bitch. How about we go slo n hard! Txt XXX SLO(4msgs)\n",
            "class: 1\n",
            " \n",
            "example  6\n",
            "PRIVATE! Your 2003 Account Statement for 078\n",
            "class: 1\n",
            " \n",
            "example  7\n",
            "Hi babe its Chloe, how r u? I was smashed on saturday night, it was great! How was your weekend? U been missing me? SP visionsms.com Text stop to stop 150p/text\n",
            "class: 1\n",
            " \n",
            "example  8\n",
            "Latest News! Police station toilet stolen, cops have nothing to go on!\n",
            "class: 1\n",
            " \n",
            "example  9\n",
            "FROM 88066 LOST å£12 HELP\n",
            "class: 1\n",
            " \n",
            "example  10\n",
            "thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse???\n",
            "class: 1\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImnMMjVDVthh",
        "colab_type": "text"
      },
      "source": [
        "**Reason analysis**: Fist of all, because I ust bags of word to vecterized the text, the model classification largely depend on the word that appears on the text. Thus if the spam text has word appearly to be a spam, it won't be classified as spam. Secondly, I didn't remove the capitalization in the text, so the word 'PREMIERE' is different from 'premiere', and some spam text escape from the classifier. Also, the website and tel-phone number is not taken account in this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ja1hoUIFp_C2"
      },
      "source": [
        "## End of Part 1.\n"
      ]
    }
  ]
}